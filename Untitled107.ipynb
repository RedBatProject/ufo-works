{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "15uIbRoy-Q_J5T1v9lIRcAjkc3ESpWG6S",
      "authorship_tag": "ABX9TyOFzh4BO7NvKdLKupJgTGfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RedBatProject/ufo-works/blob/main/Untitled107.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, TFDistilBertModel\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "vRs4eD_CXn4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Heye_lUlXDJy"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.max_len = 1500\n",
        "        self.embed_dim = 300\n",
        "        self.tokenizer = None\n",
        "        self.glove_embeddings = {}\n",
        "\n",
        "    def setup_kaggle(self):\n",
        "        !pip install -q kaggle\n",
        "        from google.colab import files\n",
        "        import os\n",
        "        print(\"Upload your kaggle API key to download DATASET\")\n",
        "        files.upload()\n",
        "        os.makedirs('~/.kaggle',exist_ok=True)\n",
        "        !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "        !kaggle datasets download -d bhavikjikadara/fake-news-detection\n",
        "\n",
        "        with zipfile.ZipFile('/content/fake-news-detection.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('data')\n",
        "\n",
        "    def load_data(self):\n",
        "        fake = pd.read_csv('data/fake.csv')\n",
        "        true = pd.read_csv('data/true.csv')\n",
        "\n",
        "        fake['label'] = 0\n",
        "        true['label'] = 1\n",
        "\n",
        "        df = pd.concat([fake, true], axis=0)\n",
        "        df = df.sample(frac=1).reset_index(drop=True)\n",
        "        return df[['text', 'label']]\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'@\\w+', '', text)  # Remove usernames\n",
        "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-English chars and punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return text.strip()\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        df['text'] = df['text'].apply(self.clean_text)\n",
        "        texts = df['text'].values\n",
        "        labels = df['label'].values\n",
        "\n",
        "        train_text, temp_text, train_label, temp_label = train_test_split(\n",
        "            texts, labels, test_size=0.3, random_state=42)\n",
        "        val_text, test_text, val_label, test_label = train_test_split(\n",
        "            temp_text, temp_label, test_size=0.5, random_state=42)\n",
        "\n",
        "        return (train_text, train_label), (val_text, val_label), (test_text, test_label)\n",
        "\n",
        "    def load_glove(self):\n",
        "        !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "        with zipfile.ZipFile('/content/glove.6B.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('data')\n",
        "        with open(\"/content/data/glove.6B.300d.txt\",'rb') as f:\n",
        "            for line in f:\n",
        "                values = line.decode().split()\n",
        "                word = values[0]\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                self.glove_embeddings[word] = coefs\n",
        "\n",
        "    def prepare_embeddings(self, train_text):\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.tokenizer.fit_on_texts(train_text)\n",
        "\n",
        "        word_index = self.tokenizer.word_index\n",
        "        vocab_size = len(word_index) + 1\n",
        "\n",
        "        embedding_matrix = np.zeros((vocab_size, self.embed_dim))\n",
        "        for word, i in word_index.items():\n",
        "            embedding_vector = self.glove_embeddings.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        return vocab_size, embedding_matrix\n",
        "\n",
        "class ModelBuilder:\n",
        "    def __init__(self, vocab_size, embedding_matrix, max_len, embed_dim=300):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.max_len = max_len\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build_model_1(self):\n",
        "        input_layer = Input(shape=(self.max_len,))\n",
        "        x = Embedding(self.vocab_size, self.embed_dim,\n",
        "                     embeddings_initializer=Constant(self.embedding_matrix),\n",
        "                     input_length=self.max_len,\n",
        "                     trainable=False)(input_layer)\n",
        "        x = Conv1D(32, 2, activation='relu')(x)\n",
        "        x = Dropout(0.6)(x)\n",
        "        x = Bidirectional(LSTM(32))(x)\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        output = Dense(1, activation='sigmoid')(x)\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_model_2(self):\n",
        "\n",
        "        input_layer = Input(shape=(self.max_len,))\n",
        "        x = Embedding(self.vocab_size, self.embed_dim,\n",
        "                     embeddings_initializer=Constant(self.embedding_matrix),\n",
        "                     input_length=self.max_len,\n",
        "                     trainable=False)(input_layer)\n",
        "        x = Conv1D(32, 2, activation='relu')(x)\n",
        "        x = Conv1D(32, 2, activation='relu')(x)\n",
        "        x = Dropout(0.6)(x)\n",
        "        x = Bidirectional(LSTM(32))(x)\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        output = Dense(1, activation='sigmoid')(x)\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_model_3(self):\n",
        "\n",
        "        input_layer = Input(shape=(self.max_len,))\n",
        "        x = Embedding(self.vocab_size, self.embed_dim,\n",
        "                     embeddings_initializer=Constant(self.embedding_matrix),\n",
        "                     input_length=self.max_len,\n",
        "                     trainable=False)(input_layer)\n",
        "        x = Conv1D(32, 2, activation='relu')(x)\n",
        "        x = Conv1D(32, 2, activation='relu')(x)\n",
        "        x = Conv1D(32, 2, activation='relu')(x)\n",
        "        x = Dropout(0.6)(x)\n",
        "        x = Bidirectional(LSTM(32))(x)\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        output = Dense(1, activation='sigmoid')(x)\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_model_4(self):\n",
        "\n",
        "        input_layer = Input(shape=(self.max_len,))\n",
        "        x = Embedding(self.vocab_size, self.embed_dim,\n",
        "                     embeddings_initializer=Constant(self.embedding_matrix),\n",
        "                     input_length=self.max_len,\n",
        "                     trainable=False)(input_layer)\n",
        "        x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
        "        x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
        "        x = Bidirectional(LSTM(32))(x)\n",
        "        x = Dropout(0.6)(x)\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        output = Dense(1, activation='sigmoid')(x)\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, max_len=1500, epochs=1):\n",
        "        self.max_len = max_len\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def train_models(self, models, train_data, val_data):\n",
        "        histories = []\n",
        "        print(\"train and evaluate Deep Learning Models ...\")\n",
        "        for model in models:\n",
        "            history = model.fit(\n",
        "                train_data[0], train_data[1],\n",
        "                validation_data=val_data,\n",
        "                epochs=self.epochs,\n",
        "                batch_size=self.batch_size,\n",
        "                callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]\n",
        "            )\n",
        "            histories.append(history)\n",
        "        return histories\n",
        "\n",
        "    def evaluate_models(self, models, test_data):\n",
        "        metrics = []\n",
        "        for model in models:\n",
        "            y_pred = model.predict(test_data[0])\n",
        "            y_pred_class = (y_pred > 0.5).astype(int)\n",
        "\n",
        "            accuracy = accuracy_score(test_data[1], y_pred_class)\n",
        "            precision = precision_score(test_data[1], y_pred_class)\n",
        "            recall = recall_score(test_data[1], y_pred_class)\n",
        "            f1 = f1_score(test_data[1], y_pred_class)\n",
        "            roc_auc = roc_auc_score(test_data[1], y_pred)\n",
        "\n",
        "            metrics.append({\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'roc_auc': roc_auc\n",
        "            })\n",
        "        return metrics\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer=None, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        if self.tokenizer:\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_len,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(0),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "                'label': label\n",
        "            }\n",
        "        else:\n",
        "            return text, label\n",
        "\n",
        "class DistilBERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(DistilBERTClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        return torch.sigmoid(self.fc(pooled_output)).squeeze()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=0, lr=2e-5):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch in tqdm(train_loader):\n",
        "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {train_loss/len(train_loader)}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    preds, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            preds.extend(outputs.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    preds = np.array(preds) > 0.5\n",
        "    accuracy = accuracy_score(true_labels, preds)\n",
        "    precision = precision_score(true_labels, preds)\n",
        "    recall = recall_score(true_labels, preds)\n",
        "    f1 = f1_score(true_labels, preds)\n",
        "    roc_auc = roc_auc_score(true_labels, preds)\n",
        "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}, ROC AUC: {roc_auc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Setup and process data\n",
        "    processor = DataProcessor()\n",
        "    processor.setup_kaggle()\n",
        "    df = processor.load_data()\n",
        "    (train_texts, train_labels), (val_texts, val_labels), (test_texts, test_labels) = processor.preprocess_data(df)\n",
        "\n",
        "    # Prepare GloVe embeddings\n",
        "    processor.load_glove()\n",
        "    vocab_size, embedding_matrix = processor.prepare_embeddings(train_text)\n",
        "\n",
        "    # Tokenize and pad sequences\n",
        "    train_seq = processor.tokenizer.texts_to_sequences(train_text)\n",
        "    val_seq = processor.tokenizer.texts_to_sequences(val_text)\n",
        "    test_seq = processor.tokenizer.texts_to_sequences(test_text)\n",
        "\n",
        "    train_pad = pad_sequences(train_seq, maxlen=processor.max_len)\n",
        "    val_pad = pad_sequences(val_seq, maxlen=processor.max_len)\n",
        "    test_pad = pad_sequences(test_seq, maxlen=processor.max_len)\n",
        "\n",
        "    # Build models\n",
        "    model_builder = ModelBuilder(vocab_size, embedding_matrix, processor.max_len)\n",
        "    models = [\n",
        "        model_builder.build_model_1(),\n",
        "        model_builder.build_model_2(),\n",
        "        model_builder.build_model_3(),\n",
        "        model_builder.build_model_4()\n",
        "    ]\n",
        "\n",
        "    # Train models\n",
        "    trainer = Trainer()\n",
        "    histories = trainer.train_models(\n",
        "        models,\n",
        "        (train_pad, train_label),\n",
        "        (val_pad, val_label)\n",
        "    )\n",
        "\n",
        "    # Evaluate models\n",
        "    metrics = trainer.evaluate_models(models, (test_pad, test_label))\n",
        "    for i, metric in enumerate(metrics):\n",
        "        print(f\"Model {i+1} Metrics:\")\n",
        "        print(metric)\n",
        "\n",
        "    # Train and evaluate DistilBERT\n",
        "\n",
        "\n",
        "    print(\"Train and evaluate DistilBERT\")\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    train_dataset = NewsDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n",
        "    val_dataset = NewsDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n",
        "    test_dataset = NewsDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    model = DistilBERTClassifier()\n",
        "    model = train_model(model, train_loader, val_loader, epochs=0)\n",
        "\n",
        "    evaluate_model(model, test_loader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cO8FNDTXmbT",
        "outputId": "baa732f3-f57f-4b6f-a8d4-1e33b1208b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file '/root/.kaggle/': Not a directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/bhavikjikadara/fake-news-detection\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "fake-news-detection.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Train and evaluate DistilBERT\n",
            "Accuracy: 0.22791388270230142, Precision: 0.20965353864376665, Recall: 0.2185859833281877, F1: 0.2140266021765417, ROC AUC: 0.22757102370070711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer=None, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        if self.tokenizer:\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_len,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(0),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "                'label': label\n",
        "            }\n",
        "        else:\n",
        "            return text, label\n",
        "\n",
        "class DistilBERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super(DistilBERTClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        return torch.sigmoid(self.fc(pooled_output)).squeeze()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=5, lr=2e-5):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch in tqdm(train_loader):\n",
        "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {train_loss/len(train_loader)}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    preds, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            preds.extend(outputs.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    preds = np.array(preds) > 0.5\n",
        "    accuracy = accuracy_score(true_labels, preds)\n",
        "    precision = precision_score(true_labels, preds)\n",
        "    recall = recall_score(true_labels, preds)\n",
        "    f1 = f1_score(true_labels, preds)\n",
        "    roc_auc = roc_auc_score(true_labels, preds)\n",
        "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}, ROC AUC: {roc_auc}\")\n",
        "\n",
        "def main():\n",
        "    df_fake = pd.read_csv('data/fake.csv')\n",
        "    df_true = pd.read_csv('data/true.csv')\n",
        "\n",
        "    df_fake['label'] = 0\n",
        "    df_true['label'] = 1\n",
        "    df = pd.concat([df_fake, df_true]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['text'], df['label'], test_size=0.3, random_state=42)\n",
        "    val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    train_dataset = NewsDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n",
        "    val_dataset = NewsDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n",
        "    test_dataset = NewsDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "    model = DistilBERTClassifier()\n",
        "    model = train_model(model, train_loader, val_loader, epochs=3)\n",
        "\n",
        "    evaluate_model(model, test_loader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "VgSW0GxG7UNf",
        "outputId": "8341755a-7478-4e62-f649-35d8397cf33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|‚ñè         | 47/1965 [00:45<30:53,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-34515444543a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-34515444543a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBERTClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-34515444543a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}, Loss: {train_loss/len(train_loader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PAdWfor97VIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}